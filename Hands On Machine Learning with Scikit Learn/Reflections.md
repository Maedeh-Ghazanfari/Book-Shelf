ðŸ“˜ Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition) by AurÃ©lien GÃ©ron

This is a personal study repo where I take notes as I work through the book. My goal is to understand ML concepts better and build a review-friendly reference for myself and others. Feel free to follow along, fork, or suggest ideas!

ðŸ§  What Iâ€™ve learned so far:

ðŸŽ¯Some Types of Noise in Data

Stochastic Noise: Random variations in data that cannot be explained by input features â€” also known as irreducible error. Itâ€™s unpredictable, not caused by the model, and inherent in real-world data.


Rounding Error: A rounding error is a deterministic type of noise that arises due to limitations in how numbers are stored or represented, especially when working with floating-point numbers. Rounding error occurs when a number cannot be represented exactly in the computer's memory, so it gets rounded to the nearest representable value. Itâ€™s systematic, meaning it's introduced in a predictable, repeatable way.

ðŸŽ¯Plots along with the correlation coefficient between their horizontal and vertical axes:

<img width="858" height="381" alt="image" src="https://github.com/user-attachments/assets/30aa8760-443d-44bb-b165-6a0a8b7f3785" />

ðŸŽ¯Feature Scaling:

<img width="838" height="293" alt="image" src="https://github.com/user-attachments/assets/74dbb2cb-594c-4c6e-9be7-2a4935615050" />

Always split before normalizing.  Normalize test data using training data stats. If you normalize before splitting, the min and max (or mean and std) used for scaling will be influenced by the test set. That means your test data is "leaking" into your training process â€” this is called data leakage, and it can lead to unrealistically good performance.

Which algorithms need scaling?

Some algorithms are sensitive to the scale or units of the features because they rely on distances or gradients; therefore, they require scaling:

k-NN uses Euclidean distance to compare samples
Logistic Regression	uses gradient descent for optimization
Linear Regression (if GD used)	Same as above
SVM (Support Vector Machine)	Distance to the hyperplane matters
K-Means Clustering	Uses distances to cluster centers
PCA	Based on variance/covariance, sensitive to scale
Neural Networks	Gradient-based; scales improve convergence

*Remember tree based models don't need scaling.
